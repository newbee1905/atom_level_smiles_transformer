name: chemformer-medium-gate
block_size: 512
n_encoder_layer: 6
n_decoder_layer: 6
n_head: 8
d_model: 512
dropout: 0.1
use_submersion: false
submersion_pooling_method: "attention"
immersion_activation: "silu"
use_kv_cache: false
use_gate: true
use_qk_norm: true
# ffn_multiplier: 2.671875  # Ensures FFN intermediate size (1368) is divisible by 8 for V100 tensor cores.
ffn_multiplier: 2.75 # Ensures FFN intermediate size (1408) is divisible by 64 for V100 tensor cores.
rope_theta: 10000.0
layer_scale_init: 0.0001
electra_task: false
encoder_attention_type: "mha"
max_relative_positions: 512
use_liger_ff: false
use_liger_norm: false
use_liger_rope: true
tie_word_embeddings: true
use_default_liger_ff: false
use_default_liger_norm: false
use_default_liger_rope: false

