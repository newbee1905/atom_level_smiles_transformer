defaults:
  - model: laptop
  - task: finetune_uspto_sep
  - _self_

# hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

wandb:
  project: "chemformer"
  name: null

model:
  vocab_path: config/vocab.yaml
  max_length: ${model.block_size}

training:
  batch_size: 128
  num_workers: 10
  epochs: 40
  optimizer:
    lr: 0.0001
    weight_decay: 0.01
  logger: "wandb" # "tensorboard" or "wandb"
  compile: true
  dtype: "bfloat16" # or "float16" or "float32"
  grad_accum_steps: 1
  grad_clip: 1.0
  early_stopping_patience: 3
  electra_loss_weight: 50.0
  softcap:
    enabled: false
    value: 30.0
