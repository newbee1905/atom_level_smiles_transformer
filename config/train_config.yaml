defaults:
  - model: laptop
  - _self_

# hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

wandb:
  project: "chemformer"
  name: null

model:
  vocab_path: config/vocab.yaml
  max_length: ${model.block_size}

dataset:
  lmdb_path: data/zinc.lmdb

training:
  batch_size: 16
  num_workers: 2
  epochs: 10
  optimizer:
    lr: 0.0001
    weight_decay: 0.01
  logger: "tensorboard" # "tensorboard" or "wandb"
  compile: false
  dtype: "bfloat16" # or "float16" or "float32"
  grad_accum_steps: 1
  grad_clip: 1.0
  early_stopping_patience: 3
  electra_loss_weight: 50.0
  softcap:
    enabled: false
    value: 30.0
