defaults:
  - model: medium
  - task: pretrain_zinc
  - _self_

# hydra configuration
hydra:
  run:
    dir: outputs/${task.name}/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}

wandb:
  project: "chemformer"
  name: null

model:
  vocab_path: config/vocab.yaml
  max_length: ${model.block_size}

training:
  batch_size: 64
  num_workers: 12
  epochs: 3
  optimizer:
    lr: 1e-6
    weight_decay: 0.01
    muon_lr_multiplier: 100.0
  logger: "tensorboard" # "tensorboard" or "wandb"
  compile: true
  dtype: "float16" # or "float16" or "float32"
  grad_accum_steps: 16
  grad_clip: 1.0
  early_stopping_patience: 3
  electra_loss_weight: 50.0
  scheduler:
    name: "linear" # "linear", "cosine", "onecycle"
    warmup_steps: 10000
    # For OneCycleLR
    max_lr: 0.01
    pct_start: 0.3
  softcap:
    enabled: false
    value: 30.0
